<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kafka笔记]]></title>
    <url>%2F2018%2F05%2F26%2Ftest%2F</url>
    <content type="text"><![CDATA[Kafka集群搭建1. kafka基本结构作为一个消息系统，其基本结构中至少要有产生消息的组件（消息生产者，Producer）以及消费消息的组件（消费者，Consumer)。生产者负责生产消息，将消息写入Kafka集群;消费者从Kafka 集群中拉取消息。 1234567graph LR 生产者1--&gt;kafka集群; 生产者2--&gt;kafka集群; 生产者3--&gt;kafka集群; kafka集群--&gt;消费者1; kafka集群--&gt;消费者2; kafka集群--&gt;消费者3; 2. kafka基本概念kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 主题Kafka 将一组消息抽象归纳为一个主题（Topic），也就是说，一个主题就是对消息的一个分类。生产者将消息发送到特定主题，消费者订阅主题或主题的某些分区进行消费。 消息消息是Kafka 通信的基本单位，由一个固定长度的消息头和一个可变长度的消息体构成。 分区和副本kafka在创建topic时，可以设置所拥有的分区和副本数，也可以在Kafka启动时所加载的配置文件中配置。每个分区在物理上对应为一个文件夹，每个分区又有一至多个副本（Replica），分区的副本分布在集群的不同代理上，以提高可用性。从存储角度上分析，分区的每个副本在逻辑上抽象为一个日志（Log）对象，即分区的副本与日志对象是一一对应的。分区使得Kafka在井发处理上变得更加容易，分区也是Kafka保证消息被顺序消费以及对消息进行负载均衡的基础。 偏移量任何发布到分区的消息会被直接追加到日志文件（分区目录下以＂.log”为文件名后缀的数据文件〉的尾部，而每条消息在日志文件中的位置都会对应一个按序递增的偏移量。Kafka 几乎不允许对消息进行随机读写，因此Kafka并没有提供额外索引机制到存储偏移量也就是说并不会给偏移量再提供索引。消费者可以通过控制消息偏移量来对消息进行消费，如消费者可以指定消费的起始偏移量。为了保证消息被顺序消费，消费者己消费的消息对应的偏移量也需要保存。 日志段一个日志又被划分为多个日志段（LogSegment），日志段是Kafka日志对象分片的最小单位。与日志对象一样，日志段也是一个逻辑概念，一个日志段对应磁盘上一个具体日志文件和两个索引文件。日志文件是以“.log”为文件名后缀的数据文件，用于保存消息实际数据。两个索引文件分别以“.index”和“.timeindex”作为文件名后缀，分别表示消息偏移量索引文件和消息时间戳索引文件。 代理在Kafka 基本体系结构提到了Kafka集群。Kafka集群就是由一个或多个Kafka实例构成，我们将每一个Kafka实例称为代理（Broker），通常也称代理为Kafka 服务器( KafkaServer ） 生产者生产者（ Producer ）负责将消息发送给代理，也就是向Kafka 代理发送消息的客户端。 消费者和消费组消费者（ C omsumer ）以拉取（ pull ）方式拉取数据，它是消费的客户端。在Kafka 中每一个消费者都属于一个特定消费组（ConsumerGroup），我们可以为每个消费者指定一个消费组，以groupld 代表消费组名称，通过group.id配置设置。如果不指定消费组，则该消费者属于默认消费组test-consumer-group。同时，每个消费者也有一个全局唯一的id，通过配置项client.id指定，如果客户端没有指定消费者的id,Kafka会自动为该消费者生成一个全局唯一的id 。同一个主题的一条消息只能被同一个消费组下某一个消费者消费，但不同消费组的消费者可同时消费该消息。消费组是Kafka用来实现对一个主题消息进行广播和单播的手段，实现消息广播只需指定各消费者均属于不同的消费组，消息单播则只需让各消费者属于同一个消费组。 3. kafka集群搭建 搭建步骤默认建立在服务器已配置好java环境以及搭建好的外部zookeeper集群。 注意：kafka自带zookeeper，应避免其与外部zookeeper发生冲突。 假设搭建好的zookeeper集群环境如下：三个节点名分别为spamaster spaslave1 spaslave2 192.168.2.161：2181 192.168.2.162：2181 192.168.2.198：2181 我们同样在161,162,198三个服务器上搭建kafka集群 安装路径/home/hadoop/bigdata/kafka 首先在spamaster节点上准备好安装包 我们选用的kafka版本为 kafka_2.10-0.10.2.1.tgz官网下载地址http://kafka.apache.org/downloads.html 将安装包解压到需要存放的目录下，例如 1tar zxvf kafka_2.10-0.10.2.1.tgz -C /home/hadoop/bigdata 1mv kafka_2.10-0.10.2.1 kafka 修改配置文件主要修改的配置文件包括：config目录下的server.properties配置文件，这是kafka配置文件 config下的zookeeper.properties文件，是用来配置kafka自带的zookeeper的，此处我们不需要修改 12345678910111213141516171819202122#spamaster节点#server.properties#kafka服务器idbroker.id=0#开启删除，保证topic被删除时不是仅仅标记删除delete.topic.enable=truelisteners=PLAINTEXT://spamaster:9092advertised.listeners=PLAINTEXT://spamaster:9092num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/home/hadoop/bigdata/kafka/logsnum.partitions=1num.recovery.threads.per.data.dir=1log.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=spamaster:2181,spaslave1:2181,spaslave2:2181zookeeper.connection.timeout.ms=6000 将配置好的kafka文件夹复制到另外两个spaslave节点相同路径下 server.properties里的如下字段要根据节点修改 broker.id spamaster：0 spaslave1：1 spaslave2: 2 listeners PLAINTEXT://spamaster:9092 PLAINTEXT://spaslave1:9092 PLAINTEXT://spaslave2:9092 advertised.listeners advertised.listeners=PLAINTEXT://spamaster:9092 advertised.listeners=PLAINTEXT://spaslave1:9092 advertised.listeners=PLAINTEXT://spaslave2:9092 4. 集群启动和测试首先保证zookeeper集群正常启动1bin/kafka-server-start.sh -daemon config/server.properties 三个节点均要启动 创建topic test，test的 分区、副本为21bin/kafka-topics.sh --create --zookeeper spamaster:2181,spaslave1:2181,spaslave2:2181 --replication-factor 2 --partitions 2 --topic test 查看topic列表1bin/kafka-topics.sh --list --zookeeperspamaster:2181,spaslave1:2181,spaslave2:2181 查看topic状态1bin/kafka-topics.sh --describe --zookeeper spamaster:2181,spaslave1:2181,spaslave2:2181 --topic test 可以通过不同节点生产，消费消息来验证集群是否搭建成功 生产消息1bin/kafka-console-producer.sh --broker-list spamaster:9092 --topic test 消费消息1bin/kafka-console-consumer.sh --zookeeper spamaster:2181,spaslave1:2181,spaslave2:2181 --topic test --from-beginning 删除topic1bin/kafka-topics.sh --delete --zookeeper spamaster:2181,spaslave1:2181,spaslave2:2181 --topic test 关闭kafka1bin/kafka-server-stop.sh Flume搭建Flume是Cloudera提供的一个高可用、高可靠、分布式的海量日志采集、聚合和传输的系统。Flume支持在日志系统中定制各类数据发送方用于收集数据，同时Flume提供对数据的简单处理，并将数据处理结果写入各种数据接收方的能力 1.安装包下载和解压下载1wget http://mirrors.tuna.tsinghua.edu.cn/apache/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz 解压1tar -zxvf apache-flume-1.8.0-bin.tar.gz 1mv apache-flume-1.8.0-bin /home/hadoop/bigdata/flume 2.环境变量配置1vi ~/.bashrc 在文件中加入 export FLUME_HOME=/home/hadoop/bigdata/flume export PATH=$PATH:$FLUME_HOME/bin 保存退出,执行source命令，使环境变量生效1source ~/.bashrc 3.flume节点配置1.对conf路径下的flume-env.sh文件进行修改，加入java环境变量 export JAVA_HOME=/home/hadoop/bigdata/java 2.针对不同的sink对象，即日志收集到何处，设置不同的接收器配置文件1.在conf目录下新建flume-hdfs.conf配置文件，指定flume收集的日志写入到hdfs中12345678910111213141516171819202122232425 1 #agent1 name 2 agent1.sources=source1 3 agent1.sinks=sink1 4 agent1.channels=channel1 5 #Spooling Directory 6 #set source1 spoolDir指定使用spool源路径，即flume监听信息源的路径 7 agent1.sources.source1.type=spooldir 8 agent1.sources.source1.spoolDir=/home/hadoop/bigdata/flume/flumetest/dir/logdfs 9 agent1.sources.source1.channels=channel110 agent1.sources.source1.fileHeader = false11 agent1.sources.source1.interceptors = i112 agent1.sources.source1.interceptors.i1.type = timestamp13 #set sink1 日志文件收集到hdfs /flume/logdfs 路径下14 agent1.sinks.sink1.type=hdfs15 agent1.sinks.sink1.hdfs.path=hdfs://spamaster:9000/flume/logdfs16 agent1.sinks.sink1.hdfs.fileType=DataStream17 agent1.sinks.sink1.hdfs.writeFormat=TEXT18 agent1.sinks.sink1.hdfs.rollInterval=119 agent1.sinks.sink1.channel=channel120 agent1.sinks.sink1.hdfs.filePrefix=%Y-%m-%d21 agent1.sinks.sink1.hdfs.fileSuffix=.txt22 #set channel123 agent1.channels.channel1.type=file24 agent1.channels.channel1.checkpointDir=/home/hadoop/bigdata/flume/flumetest/dir/logdfstmp/point25 agent1.channels.channel1.dataDirs=/home/hadoop/bigdata/flume/flumetest/dir/logdfstmp 在flume路径下启动flume，指定以flume-hdfs.conf配置文件启动，命令中的agent1表示配置文件中的Agent的Name bin/flume-ng agent –conf conf –conf-file conf/flume-hdfs.conf –name agent1 -Dflume.root.logger=INFO,console 在/home/hadoop/bigdata/flume/flumetest/dir/logdfs路径下导入一个文件test.log，该log文件会被flume检测到，在文件名后添加.COMPLETED后缀并上传到hdfs 2.在conf目录下新建flume-kafka.conf配置文件，指定flume收集的日志写入到kafka中1234567891011121314151617181920212223242526 1 # 指定Agent的组件名称 2 agent1.sources = source1 3 agent1.sinks = sink1 4 agent1.channels = channel1 5 6 # 指定Flume source(要监听的路径) 7 agent1.sources.source1.type = spooldir 8 agent1.sources.source1.spoolDir = /home/hadoop/bigdata/flume/flumetest/dir/logkafka 9 agent1.sources.source1.ignorePattern = ^(.)*\\.tmp$10 11 # 指定Flume sink 12 # agent1.sinks.sink1.type = logger 13 agent1.sinks.sink1.type = org.apache.flume.sink.kafka.KafkaSink14 agent1.sinks.sink1.topic = callstatus15 agent1.sinks.sink1.brokerList = 192.168.2.198:9092,192.168.2.161:909216 agent1.sinks.sink1.requiredAcks = 117 agent1.sinks.sink1.batchSize = 1638418 19 # 指定Flume channel 20 agent1.channels.channel1.type=file21 agent1.channels.channel1.checkpointDir=/home/hadoop/bigdata/flume/flumetest/dir/logkafkatmp/point22 agent1.channels.channel1.dataDirs=/home/hadoop/bigdata/flume/flumetest/dir/logkafkatmp23 24 # 绑定source和sink到channel上 25 agent1.sources.source1.channels = channel126 agent1.sinks.sink1.channel = channel1 启动flume bin/flume-ng agent –conf conf –conf-file conf/flume-kafka.conf –name agent1 -Dflume.root.logger=INFO,console 启动kafka消费者 bin/kafka-console-consumer.sh –zookeeper spamaster:2181,spaslave1:2181,spaslave2:2181 –topic callstatus –from-beginning 在/home/hadoop/bigdata/flume/flumetest/dir/logkafka路径下导入文件，立即被kafka所消费，通过消费者程序消费flume配置中设置的topic可以看到，文件内容已经传到kafka中了 Spool用于监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点：拷贝到spool目录下的文件不可以再打开编辑(这样收集到的日志内容为空，编辑的内容无效）、spool目录下不可包含相应的子目录。]]></content>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
</search>
